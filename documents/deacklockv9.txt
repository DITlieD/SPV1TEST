The deadlock issue is resolved, and the system is now correctly utilizing the centralized queue for sequential execution. The low CPU usage (10%) is expected behavior, as the system is now constrained to sequential, single-threaded execution within the Forge worker to maintain stability.

However, the logs indicate that the Forge training cycles are failing internally. I have analyzed the codebase and identified several critical issues related to data integrity, library compatibility (DEAP and Scikit-learn), and control flow that are causing these failures.

Here is the comprehensive implementation of the required fixes.

### 1\. Data Integrity and Sanitization (Critical)

We must rigorously clean data (NaNs/Infinities) generated by complex features (like CEEMDAN and Particle Filters) before they reach the ML models.

**File:** `forge/data_processing/feature_factory.py` (Forge folder files)

```python
# In forge/data_processing/feature_factory.py
import pandas as pd
import numpy as np
# ... other imports (logging, CEEMDAN, MarketStateTracker)

class FeatureFactory:
    # ... (__init__ and other methods)

    def _generate_particle_filter_features(self, df: pd.DataFrame) -> pd.DataFrame:
        # (Ensure MarketStateTracker is imported)
        from forge.strategy.particle_filter import MarketStateTracker

        # Dynamically detect ATR column
        atr_col = next((c for c in df.columns if c.startswith('ATRr_') or c.startswith('ATR_')), None)
        
        if not atr_col:
            # (Logic to calculate ATR if missing, or return fallback if impossible)
            # ...
            return df # Return fallback if ATR cannot be determined

        # CRITICAL FIX: Handle potential division by zero and initial NaNs
        process_uncertainty = df[atr_col] / df['close']
        
        # Replace infinities and robustly fill NaNs
        process_uncertainty.replace([np.inf, -np.inf], 0, inplace=True)
        process_uncertainty.fillna(method='bfill', inplace=True)
        process_uncertainty.fillna(method='ffill', inplace=True) 

        returns = df['close'].pct_change().fillna(0).values
        returns[np.isinf(returns)] = 0
        
        # (Particle Filter loop: pf.step logic)
        pf = MarketStateTracker()
        denoised_states, state_uncertainties = [], []
        for i in range(len(returns)):
            denoised_state = pf.step(returns[i], process_uncertainty.iloc[i])
            denoised_states.append(denoised_state[0])
            state_uncertainties.append(pf.get_state_uncertainty())
        
        df['denoised_close_pct_change'] = pd.Series(denoised_states, index=df.index)
        df['state_uncertainty'] = pd.Series(state_uncertainties, index=df.index)
        
        # CRITICAL FIX: Robustly find the starting close price
        start_close = df['close'].bfill().iloc[0] if not df['close'].bfill().empty else np.nan
        
        if np.isfinite(start_close):
             df['denoised_close'] = start_close * (1 + df['denoised_close_pct_change']).cumprod()
        else:
             df['denoised_close'] = df['close'] # Fallback

        return df

    # Update _process_single_asset with final sanitization
    def _process_single_asset(self, df: pd.DataFrame) -> pd.DataFrame:
        # ... (Preserve OHLCV columns)
        ohlcv_cols = df[['open', 'high', 'low', 'close', 'volume']].copy()

        # ... (Run Technical Indicators, CEEMDAN, Particle Filter) ...
        
        numeric_df = df.select_dtypes(include=np.number)
        
        # --- FINAL SANITIZATION (CRITICAL FIX) ---
        numeric_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        
        # Interpolate internal NaNs (more robust than simple fill)
        interpolation_method = 'time' if isinstance(df.index, pd.DatetimeIndex) else 'linear'
        try:
            numeric_df.interpolate(method=interpolation_method, limit_direction='both', axis=0, inplace=True)
        except Exception as e:
            self.log(f"  -> WARNING: Interpolation failed. Error: {e}. Falling back.")

        # Final fill for edges
        numeric_df.fillna(method='ffill', inplace=True)
        numeric_df.fillna(method='bfill', inplace=True)
        numeric_df.fillna(0, inplace=True) # Absolute fallback
        # ---------------------------------------

        final_df = pd.concat([ohlcv_cols, numeric_df], axis=1)
        return final_df.loc[:,~final_df.columns.duplicated()]
```

### 2\. Fix Library Compatibility Issues

#### A. DEAP Initialization Conflicts (GP 2.0)

The 'spawn' multiprocessing context causes modules to be re-imported, conflicting with DEAP's global `creator`.

**File:** `forge/evolution/strategy_synthesizer.py` (Forge folder files)

```python
# In forge/evolution/strategy_synthesizer.py
from deap import base, creator, tools, gp, algorithms
import logging
import multiprocessing
# Ensure setup_gp_primitives is imported

class StrategySynthesizer:
    def __init__(self, feature_names, fitness_evaluator, population_size=50, generations=20, logger=None):
        # ... (Initialization of attributes) ...
        self.logger = logger if logger else logging.getLogger(__name__)
        self.pool = None

        # --- DEAP Setup ---
        self.pset = setup_gp_primitives(self.feature_names)

        # CRITICAL FIX: Robust Creator Initialization for 'spawn' context
        # Check if the types already exist before attempting to create them.
        if not hasattr(creator, "FitnessMax"):
            try:
                creator.create("FitnessMax", base.Fitness, weights=(1.0,))
            except Exception:
                pass # Handle potential race condition during import

        if not hasattr(creator, "Individual"):
            try:
                if hasattr(creator, "FitnessMax"):
                   creator.create("Individual", gp.PrimitiveTree, fitness=creator.FitnessMax)
                else:
                   self.logger.error("Cannot create DEAP Individual because FitnessMax is missing.")
            except Exception:
                 pass
        
        self.toolbox = base.Toolbox()
        
        # (Ensure all toolbox registrations are present: expr, individual, population, evaluate, select, etc.)
        # ...

        # (Multiprocessing map registration logic remains the same as previously fixed)
```

#### B. Fix `CalibratedClassifierCV` Initialization (`TypeError`)

If the Scikit-learn version does not accept `n_jobs` in `CalibratedClassifierCV`, it causes a `TypeError`. We must remove this argument.

**File:** `models_v2.py` (Main folder files)

```python
# In models_v2.py
import multiprocessing
from sklearn.calibration import CalibratedClassifierCV
# ... (LGBMWrapper and XGBWrapper imports)

class LGBMWrapper(BaseBatchModel):
    def fit(self, df_train, df_val=None, device='cpu', generation=0):
        X_train, y_train = self._prepare_data(df_train)
        
        is_subprocess = multiprocessing.current_process().name != 'MainProcess'
        # Determine n_jobs for the base model only
        model_n_jobs = 1 if is_subprocess else -1

        # (Setup default_params using model_n_jobs)
        # ...
        
        base_model = lgb.LGBMClassifier(**final_params)
        
        if df_val is not None:
            # ...
            # CRITICAL FIX: Remove n_jobs argument
            self.model = CalibratedClassifierCV(base_model, method='isotonic', cv='prefit')
            self.model.fit(X_val, y_val)
        else:
            # CRITICAL FIX: Remove n_jobs argument
            self.model = CalibratedClassifierCV(base_model, method='isotonic', cv=3)
            self.model.fit(X_train, y_train)
            
        self.is_trained = True
        return self

# Apply the identical fix structure to XGBWrapper.fit()
class XGBWrapper(BaseBatchModel):
     def fit(self, df_train, df_val=None, device='cpu', generation=0):
        # ... (Setup base model with model_n_jobs) ...

        if df_val is not None:
            # ...
            # CRITICAL FIX: Remove n_jobs argument
            self.model = CalibratedClassifierCV(base_model, method='isotonic', cv='prefit')
            self.model.fit(X_val, y_val)
        else:
            # CRITICAL FIX: Remove n_jobs argument
            self.model = CalibratedClassifierCV(base_model, method='isotonic', cv=3)
            self.model.fit(X_train, y_train)
            
        self.is_trained = True
        return self
```

### 3\. Improve Control Flow and Data Handling

#### A. Robust Data Handling in `task_scheduler.py`

Ensure DataFrames are independent (using `.copy()`) and verify integrity before training.

**File:** `forge/overlord/task_scheduler.py` (Forge folder files)

```python
# In forge/overlord/task_scheduler.py -> run_single_forge_cycle

def run_single_forge_cycle(...):
    # ... (Delayed Imports and Initialization) ...

    try:
        # ... (FeatureFactory execution) ...
        
        # Ensure the correct dataframe is extracted and independent
        if asset_symbol not in processed_universe or processed_universe[asset_symbol].empty:
            raise ValueError(f"FeatureFactory did not return data for the target asset: {asset_symbol}")
            
        df_full_features = processed_universe[asset_symbol].copy()

        # --- 2. Labeling and Data Splitting ---
        # ... (Labeling logic) ...
        df_full_features.dropna(inplace=True)

        # --- DATA SANITIZATION CHECKPOINT ---
        import numpy as np # Ensure numpy is available (from delayed imports)
        numeric_features = df_full_features.select_dtypes(include=np.number)
        if not np.isfinite(numeric_features.values).all():
             # (Implement emergency cleaning and recheck as detailed in previous responses)
             raise ValueError("Persistent data corruption (NaN/Inf) detected. Halting Forge cycle.")
        # ------------------------------------

        # Split data
        train_size = int(len(df_full_features) * 0.8)
        
        # CRITICAL FIX: Use .copy() to ensure independence
        df_train_full = df_full_features.iloc[:train_size].copy()
        df_gauntlet_full = df_full_features.iloc[train_size:].copy()
        df_train_numeric = df_train_full.select_dtypes(include=np.number).copy()

        # ... (Continue to Bake-Off) ...
        
        # --- 5. The Gauntlet ---
        # CRITICAL FIX: Handle case where no models survived the Bake-Off
        if not challengers:
            logger.warning("[Gauntlet] No challengers survived the Bake-Off. Forge cycle ending.")
            reporter.set_status("Complete", "Bake-Off yielded no viable models.")
            return None # Exit the function gracefully
        
        # ... (Gauntlet and Registration logic) ...
        
        # Ensure function returns the model_id on success or None on failure
        if best_model_info and model_id:
            return model_id
        else:
            return None

    except Exception as e:
        # ... (Error handling)
        return None
```

#### B. Fix `CrucibleEngine` Result Handling (`TypeError`)

Robustly handle cases where the Forge worker returns `None` due to failure.

**File:** `crucible_engine.py` (Main folder files)

```python
# In crucible_engine.py -> CrucibleEngine._forge_processing_loop

    async def _forge_processing_loop(self):
        # ... (loop setup)
        loop = asyncio.get_running_loop()
        while not self.stop_event.is_set():
            try:
                # (Get task from queue)
                # ...

                try:
                    if self.executor:
                        # Run the task
                        result = await loop.run_in_executor(
                            self.executor,
                            run_forge_process,
                            symbol,
                            agent_id,
                            generation
                        )
                        
                        # CRITICAL FIX: Check if the result is valid and iterable before unpacking
                        # forge_worker returns (symbol, new_model_id) on success, or None on failure.
                        if result and isinstance(result, tuple) and len(result) == 2:
                            returned_symbol, new_agent_id = result
                            
                            # Check if the new_agent_id is valid (it might be None if the Forge cycle completed but registered no model)
                            if new_agent_id:
                                self.logger.info(f"Forge task {task_id} completed. Initializing Agent {new_agent_id}...")
                                # Ensure V3Agent and CrucibleAgent are imported
                                from forge.core.agent import V3Agent
                                from forge.crucible.arena_manager import CrucibleAgent
                                
                                new_v3_agent = V3Agent(symbol=returned_symbol, exchange=self.exchange, app_config=self.config)
                                new_crucible_agent = CrucibleAgent(agent_id=new_agent_id, v3_agent=new_v3_agent)
                                self.arena.add_agent(returned_symbol, new_crucible_agent)
                                self.logger.info(f"AGENT {new_agent_id} IS LIVE and has been added to the arena.")
                            else:
                                self.logger.info(f"Forge task {task_id} completed successfully, but no new model was registered (e.g., failed Gauntlet).")
                        else:
                            # This handles the case where result is None (indicating exception in forge_worker.py)
                            self.logger.error(f"Forge task {task_id} failed or returned an invalid result (Result: {result}).")
                    else:
                         self.logger.error("Executor not available.")

                except Exception as e:
                    # Catch errors during execution OR handling of the result
                    self.logger.error(f"Error during execution or handling of Forge task {task_id}: {e}", exc_info=True)
                finally:
                    # Mark task as done
                    self.forge_queue.task_done()
                    if task_id in self.active_forge_tasks:
                        self.active_forge_tasks.remove(task_id)

            # ... (exception handling for the loop itself) ...
```